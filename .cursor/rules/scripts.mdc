---
description: Rules for writing scripts in script_builder repository
globs: scripts/*.py
alwaysApply: false
---
# Script Builder Rules

## Project Structure
- **Scripts**: Create `.py` files in `scripts/` directory (not root)
- **Utils**: Reusable code goes in `utils/` (only if used by multiple scripts)
- **Inputs**: Place input files in `inputs/` directory
- **Outputs**: Final results only in `outputs/` directory
- **Cache**: Interim steps, step logs, and token tracking as JSON with timestamps
- **Environment**: Store API keys in `.env` file (ANTHROPIC_API_KEY, GEMINI_API_KEY)

## Package Management
- **Install project**: `uv sync` (makes utils/ importable, installs dependencies)
- **Run scripts**: `uv run scripts/script_name.py`
- **Add packages**: `uv add package_name`
- **Never use pip**: Always use `uv` for dependency management

## Imports Pattern (CRITICAL)
**ALWAYS put ALL imports at the top of the file. NEVER use inline imports.**

```python
#!/usr/bin/env python3
"""Script description"""

from pathlib import Path
from datetime import datetime
from pydantic import BaseModel
from typing import List, Optional
from dotenv import load_dotenv

load_dotenv()

from utils import call_anthropic, AIRequest, AnthropicModel, save_json, extract
from utils.step_logger import StepLogger
```

**Key points:**
- ‚ùå No `sys.path.insert()` - project configured with pyproject.toml
- ‚úÖ Load dotenv before importing utils
- ‚úÖ All imports at top, never inline
- ‚úÖ Group: stdlib, third-party, local (utils)

## Step Logging (REQUIRED)
- **Always use StepLogger**: Tracks steps, progress, AND token usage
- **Initialize**: `logger = StepLogger("script_name")` at script start
- **Track steps**: `logger.step()`, `logger.output()`, `logger.update()`
- **Finalize**: Call `logger.finalize()` at end (saves to cache/)
- **Descriptive names**: Use meaningful step names like "Video Analysis" not "Step 1"

```python
logger = StepLogger("my_script")

# Start a step
logger.step("Process Data", inputs={"count": 100})

# ... do work ...
for i in range(100):
    logger.update({"progress": f"{i+1}/100"})

# Complete step with outputs
logger.output({"processed": 100})

# At script end
logger.finalize()  # Prints summary, saves step log to cache/
```

## LLM Calls (REQUIRED PATTERN)
- **Never initialize clients directly**: No `anthropic.Anthropic()` or `genai.Client()`
- **Always pass logger**: `call_anthropic(request, logger)` for token tracking
- **Use descriptive step_name**: In AIRequest for clarity in logs

```python
# Anthropic Claude
request = AIRequest(
    messages=[{"role": "user", "content": "..."}],
    model=AnthropicModel.CLAUDE_SONNET_4,
    max_tokens=1000,
    step_name="Descriptive Step Name"
)
response = call_anthropic(request, logger)

# Google Gemini
request = AIRequest(
    messages=[{"role": "user", "content": "..."}],
    model=GeminiModel.GEMINI_2_5_FLASH,
    provider=Provider.GOOGLE,
    max_tokens=1000,
    step_name="Analysis"
)
response = call_gemini(request, logger)
```

## Structured Extraction
Use `extract()` for extracting structured data with Gemini:

```python
from utils import extract

class Product(BaseModel):
    name: str
    price: float
    features: List[str]

products = extract(
    text=text,
    schema=Product,
    logger=logger,
    return_list=True,
    step_name="Extract Products"
)
```

## Data Models (REQUIRED PATTERN)
- **ALWAYS use Pydantic**: All structured data MUST use `BaseModel`
- **NEVER use**: TypedDict, dataclasses, NamedTuples, or plain dicts for structured data
- **Type hints**: All function parameters and return types
- **Validation**: Let Pydantic handle validation automatically
- **Serialization**: Use `.model_dump()` for JSON, `.model_validate()` for parsing

```python
from pydantic import BaseModel, Field
from typing import List, Optional

class Result(BaseModel):
    field: str
    score: float
    items: List[str] = Field(default_factory=list)
    metadata: Optional[dict] = None
```

**Why Pydantic:**
- Runtime validation and type checking
- Better error messages
- Consistent with AI utils (AIRequest, AIResponse)
- Automatic serialization/deserialization

## File I/O Pattern
- **Cache for interim steps**: Analysis, processing logs, intermediate results, **step logs**
- **Outputs for finals**: Videos, reports, final artifacts only
- **IMPORTANT**: Step logs (with token tracking) ALWAYS go to `cache/` (automatic)
- **Always timestamp**: Use `datetime.now().strftime("%Y%m%d_%H%M%S")`
- **Use utils**: `save_json()` and `load_json()` for consistency

```python
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Interim step to cache
save_json(data, f"analysis_{timestamp}.json", output_dir="cache", description="Analysis")

# Final result to outputs
save_json(final, f"result_{timestamp}.json", output_dir="outputs", description="Final")

# Step log with token tracking saves automatically to cache/
logger.finalize()
```

## Available Models

### Anthropic
- **Default**: `AnthropicModel.CLAUDE_SONNET_4` (balanced)
- **Best quality**: `AnthropicModel.CLAUDE_OPUS_4` (slower, expensive)
- **Fast/cheap**: `AnthropicModel.CLAUDE_3_HAIKU`
- **Previous gen**: `AnthropicModel.CLAUDE_SONNET_3_5`

### Google Gemini
- **Default**: `GeminiModel.GEMINI_2_5_FLASH` (fast, balanced)
- **Best quality**: `GeminiModel.GEMINI_2_5_PRO`
- **Very fast**: `GeminiModel.GEMINI_2_5_FLASH_LITE`
- **Previous gen**: `GeminiModel.GEMINI_2_0_FLASH`

## Script Structure Template
```python
#!/usr/bin/env python3
"""Script description"""

from pathlib import Path
from datetime import datetime
from pydantic import BaseModel
from typing import List, Optional
from dotenv import load_dotenv

load_dotenv()

from utils import call_anthropic, AIRequest, AnthropicModel, save_json
from utils.step_logger import StepLogger

class MyResult(BaseModel):
    field: str
    score: float

def main():
    logger = StepLogger("my_script")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    print("üîç Starting...")

    # Step 1: Load inputs
    logger.step("Load Data", inputs={"source": "..."})
    # ... load data ...
    logger.output({"loaded": 10})

    # Step 2: Process with LLM
    logger.step("Process Data", inputs={"count": 10})
    request = AIRequest(
        messages=[{"role": "user", "content": "..."}],
        model=AnthropicModel.CLAUDE_SONNET_4,
        max_tokens=1000,
        step_name="Analysis"
    )
    response = call_anthropic(request, logger)
    logger.output({"result": response.content})

    # Step 3: Save final
    logger.step("Save Results")
    save_json({"final": "result"}, f"result_{timestamp}.json",
              output_dir="outputs", description="Final")
    logger.output({"saved": True})

    # Finalize - prints summary and saves step log to cache/
    logger.finalize()
    print("‚úÖ Complete!")

if __name__ == "__main__":
    main()
```

## Logging & Visibility
- **StepLogger provides**: üîπ step start, üìä token usage, ‚úÖ success, ‚ùå failure
- **Add user-facing prints**: üîç analysis, ü§ñ LLM calls, üíæ saving
- **Progress updates**: Use `logger.update()` in loops for visibility
- **Token tracking**: Automatic when passing logger to LLM calls

## Error Handling
- **Fail fast**: Check inputs early, exit with clear messages
- **Use logger.fail()**: Marks step as failed and logs error details
- **Descriptive errors**: Tell user what went wrong and what to do
- **Try-except**: Wrap risky operations, use logger.fail() on exceptions

```python
# Early validation
if not input_path.exists():
    print(f"‚ùå Input not found: {input_path}")
    return

# Error handling in steps
try:
    result = process_data()
    logger.output({"result": result})
except Exception as e:
    logger.fail(e)
    print(f"‚ùå Failed: {e}")
```

## DON'Ts
- ‚ùå Don't use `sys.path.insert()` - project configured with pyproject.toml
- ‚ùå Don't put imports inline or at bottom - **ALWAYS at top**
- ‚ùå Don't initialize provider clients directly (anthropic.Anthropic(), genai.Client())
- ‚ùå Don't save interim steps to `outputs/` (only finals)
- ‚ùå Don't forget to initialize StepLogger and call `.finalize()`
- ‚ùå Don't use generic step names ("Step 1", "LLM Call") - be descriptive
- ‚ùå Don't hard-code API keys - use env vars
- ‚ùå Don't forget timestamps in filenames
- ‚ùå Don't put task-specific code in utils prematurely
- ‚ùå Don't forget to call `load_dotenv()` before importing utils
- ‚ùå Don't use TypedDict, dataclasses, or plain dicts - ALWAYS use Pydantic BaseModel
