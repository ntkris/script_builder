---
description: Rules for writing scripts in script_builder repository
globs: scripts/*.py
alwaysApply: false
---
# Script Builder Rules

## Project Structure
- **Scripts**: Create `.py` files in `scripts/` directory (not root)
- **Utils**: Reusable code goes in `utils/` (only if used by multiple scripts)
- **Inputs**: Place input files in `inputs/` directory
- **Outputs**: Final results only in `outputs/` directory
- **Cache**: Step logs (with token tracking) automatically saved by StepLogger
- **Environment**: Store API keys in `.env` file (ANTHROPIC_API_KEY, GEMINI_API_KEY)

**Logging Philosophy:**
- **StepLogger is your single source of truth** - everything goes there
- Log **actual data and decisions**, not just summary statistics
- Only create separate cache files if data is too large or needs reuse

## Package Management
- **Install project**: `uv sync` (makes utils/ importable, installs dependencies)
- **Run scripts**: `uv run scripts/script_name.py`
- **Add packages**: `uv add package_name`
- **Never use pip**: Always use `uv` for dependency management
- **Clear cache**: `uv run scripts/clear_cache.py` (clears all cache files including resume state)

## Imports Pattern (CRITICAL)
**ALWAYS put ALL imports at the top of the file. NEVER use inline imports.**

```python
#!/usr/bin/env python3
"""Script description"""

from pathlib import Path
from datetime import datetime
from pydantic import BaseModel
from typing import List, Optional
from dotenv import load_dotenv

load_dotenv()

from utils import call_anthropic, AIRequest, AnthropicModel, save_json, extract
from utils.step_logger import StepLogger
```

**Key points:**
- ‚ùå No `sys.path.insert()` - project configured with pyproject.toml
- ‚úÖ Load dotenv before importing utils
- ‚úÖ All imports at top, never inline
- ‚úÖ Group: stdlib, third-party, local (utils)

## Step Logging (REQUIRED)
- **Always use StepLogger**: Tracks steps, progress, AND token usage
- **Initialize**: `logger = StepLogger("script_name")` at script start
- **Track steps**: `logger.step()`, `logger.output()`, `logger.update()`
- **Finalize**: Call `logger.finalize()` at end (saves to cache/)
- **Descriptive names**: Use meaningful step names like "Video Analysis" not "Step 1"

**CRITICAL - Log Rich Data:**
- Log **actual data** in inputs/outputs, not just counts
- Include **decisions, reasons, and extractions** - everything needed to debug
- Summary stats are OK as a supplement, but never replace actual data
- Think: "Can I debug this problem with just the StepLogger JSON?"

```python
logger = StepLogger("my_script")

# Start a step - log actual inputs
logger.step("Evaluate Items", inputs={
    "items": [{"id": 1, "name": "..."}, {"id": 2, "name": "..."}],
    "threshold": 0.8
})

# ... do work ...
results = []
for i, item in enumerate(items):
    result = evaluate(item)
    results.append(result)
    logger.update({"progress": f"{i+1}/{len(items)}", "current": item.id})

# Log actual results with decisions, not just count
logger.output({
    "evaluations": [
        {"id": 1, "score": 0.9, "decision": "accepted", "reason": "..."},
        {"id": 2, "score": 0.6, "decision": "rejected", "reason": "..."}
    ],
    "stats": {"total": 2, "accepted": 1, "rejected": 1}  # Stats supplement data
})

# At script end
logger.finalize()  # Prints summary, saves step log to cache/
```

## Resumable Scripts (for long-running operations)

**When to use resumable mode:**
- Processing 50+ items that take time (API calls, web scraping)
- Operations that might timeout or crash
- Expensive operations you don't want to repeat

**When NOT to use:**
- Simple scripts with < 10 items
- Fast operations (< 1 minute total)
- Scripts that need fresh data every run

### Pattern for resumable scripts:

```python
import argparse
from utils.step_logger import StepLogger

def build_argument_parser():
    parser = argparse.ArgumentParser(description="...")
    # ... other args ...
    parser.add_argument("--fresh", action="store_true",
                       help="Start fresh, ignore resume state")
    return parser

def main():
    args = parser.parse_args()

    # Enable resumable mode
    logger = StepLogger("script_name", resumable=True)

    # Handle --fresh flag
    if args.fresh and logger.resume_state_file.exists():
        logger.resume_state_file.unlink()
        print("üóëÔ∏è  Cleared resume state, starting fresh")

    # Step 1: Generate or load data
    logger.step("Load Data")
    if logger.should_run_step():
        data = expensive_operation()
        logger.output({"data": data})
    else:
        data = logger.get_cached_output("data")
        print("‚úÖ Loaded from cache")

    # Step 2: Process items with resume support
    logger.step("Process Items")

    # Load intermediate state if resuming
    if logger.should_run_step():
        results = []
    else:
        results = logger.get_cached_output("results") or []

    for item in items:
        # Skip already processed items
        if logger.is_item_completed(item.id):
            print(f"‚è≠Ô∏è  Skipping {item.id}")
            continue

        result = process_item(item)
        results.append(result)

        # Mark complete and save immediately (crash-safe)
        logger.mark_item_complete(item.id)

    # Save intermediate state in output for resumability
    logger.output({
        "results": results,
        "intermediate_data": other_data  # Include what's needed to resume
    })

    # Step 3: Save final output
    logger.step("Save Results")
    if logger.should_run_step():
        save_results(results)
        logger.output({"saved": True})

    logger.finalize()  # Clears resume state on success
```

**Key methods:**
- `should_run_step()` - Returns False if step already completed
- `get_cached_output(key)` - Loads data from previous run
- `is_item_completed(item_id)` - Checks if item was processed
- `mark_item_complete(item_id)` - Saves state immediately (crash-safe)

**How it works:**
- Creates `cache/resume_state_{script_name}.json` with completed steps and items
- Each `mark_item_complete()` saves immediately (survives crashes)
- Resume state auto-deleted on successful `finalize()`
- Use `--fresh` flag to force clean run
- Use `uv run scripts/clear_cache.py` to manually clear all cache including resume state

## LLM Calls (REQUIRED PATTERN)
- **Never initialize clients directly**: No `anthropic.Anthropic()` or `genai.Client()`
- **Always pass logger**: `call_anthropic(request, logger)` for token tracking
- **Use descriptive step_name**: In AIRequest for clarity in logs

```python
# Anthropic Claude
request = AIRequest(
    messages=[{"role": "user", "content": "..."}],
    model=AnthropicModel.CLAUDE_SONNET_4,
    max_tokens=1000,
    step_name="Descriptive Step Name"
)
response = call_anthropic(request, logger)

# Google Gemini
request = AIRequest(
    messages=[{"role": "user", "content": "..."}],
    model=GeminiModel.GEMINI_2_5_FLASH,
    provider=Provider.GOOGLE,
    max_tokens=1000,
    step_name="Analysis"
)
response = call_gemini(request, logger)
```

## Structured Extraction
Use `extract()` for extracting structured data with Gemini:

```python
from utils import extract

class Product(BaseModel):
    name: str
    price: float
    features: List[str]

products = extract(
    text=text,
    schema=Product,
    logger=logger,
    return_list=True,
    step_name="Extract Products"
)
```

## Data Models (REQUIRED PATTERN)
- **ALWAYS use Pydantic**: All structured data MUST use `BaseModel`
- **NEVER use**: TypedDict, dataclasses, NamedTuples, or plain dicts for structured data
- **Type hints**: All function parameters and return types
- **Validation**: Let Pydantic handle validation automatically
- **Serialization**: Use `.model_dump()` for JSON, `.model_validate()` for parsing

```python
from pydantic import BaseModel, Field
from typing import List, Optional

class Result(BaseModel):
    field: str
    score: float
    items: List[str] = Field(default_factory=list)
    metadata: Optional[dict] = None
```

**Why Pydantic:**
- Runtime validation and type checking
- Better error messages
- Consistent with AI utils (AIRequest, AIResponse)
- Automatic serialization/deserialization

## File I/O Pattern
- **StepLogger for all data**: Log everything to StepLogger inputs/outputs (single source of truth)
- **Outputs for final deliverables**: Videos, reports, final artifacts only
- **IMPORTANT**: Step logs (with token tracking) ALWAYS go to `cache/` (automatic)
- **Avoid separate cache files**: Only create if data is too large or needs reuse
- **Always timestamp**: Use `datetime.now().strftime("%Y%m%d_%H%M%S")`

```python
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# PREFER: Log to StepLogger instead of separate files
logger.step("Analyze Data", inputs={"items": [...]})
logger.output({"results": [...]})  # All data here for debugging

# ONLY create separate files for:
# 1. Final deliverables
save_json(final, f"report_{timestamp}.json", output_dir="outputs", description="Final Report")

# 2. Very large data (page text, images, etc.)
# 3. Data that needs to be reused by another process

# Step log with ALL your data saves automatically to cache/
logger.finalize()
```

## Available Models

### Anthropic
- **Default**: `AnthropicModel.CLAUDE_SONNET_4` (balanced)
- **Best quality**: `AnthropicModel.CLAUDE_OPUS_4` (slower, expensive)
- **Fast/cheap**: `AnthropicModel.CLAUDE_3_HAIKU`
- **Previous gen**: `AnthropicModel.CLAUDE_SONNET_3_5`

### Google Gemini
- **Default**: `GeminiModel.GEMINI_2_5_FLASH` (fast, balanced)
- **Best quality**: `GeminiModel.GEMINI_2_5_PRO`
- **Very fast**: `GeminiModel.GEMINI_2_5_FLASH_LITE`
- **Previous gen**: `GeminiModel.GEMINI_2_0_FLASH`

## Script Structure Template
```python
#!/usr/bin/env python3
"""Script description"""

from pathlib import Path
from datetime import datetime
from pydantic import BaseModel
from typing import List, Optional
from dotenv import load_dotenv

load_dotenv()

from utils import call_anthropic, AIRequest, AnthropicModel, save_json
from utils.step_logger import StepLogger

class MyResult(BaseModel):
    field: str
    score: float

def main():
    logger = StepLogger("my_script")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    print("üîç Starting...")

    # Step 1: Load inputs
    logger.step("Load Data", inputs={"source": "..."})
    # ... load data ...
    logger.output({"loaded": 10})

    # Step 2: Process with LLM
    logger.step("Process Data", inputs={"count": 10})
    request = AIRequest(
        messages=[{"role": "user", "content": "..."}],
        model=AnthropicModel.CLAUDE_SONNET_4,
        max_tokens=1000,
        step_name="Analysis"
    )
    response = call_anthropic(request, logger)
    logger.output({"result": response.content})

    # Step 3: Save final
    logger.step("Save Results")
    save_json({"final": "result"}, f"result_{timestamp}.json",
              output_dir="outputs", description="Final")
    logger.output({"saved": True})

    # Finalize - prints summary and saves step log to cache/
    logger.finalize()
    print("‚úÖ Complete!")

if __name__ == "__main__":
    main()
```

## Logging & Visibility
- **StepLogger provides**: üîπ step start, üìä token usage, ‚úÖ success, ‚ùå failure
- **Add user-facing prints**: üîç analysis, ü§ñ LLM calls, üíæ saving
- **Progress updates**: Use `logger.update()` in loops for visibility
- **Token tracking**: Automatic when passing logger to LLM calls

## Error Handling
- **Fail fast**: Check inputs early, exit with clear messages
- **Use logger.fail()**: Marks step as failed and logs error details
- **Descriptive errors**: Tell user what went wrong and what to do
- **Try-except**: Wrap risky operations, use logger.fail() on exceptions

```python
# Early validation
if not input_path.exists():
    print(f"‚ùå Input not found: {input_path}")
    return

# Error handling in steps
try:
    result = process_data()
    logger.output({"result": result})
except Exception as e:
    logger.fail(e)
    print(f"‚ùå Failed: {e}")
```

## DON'Ts
- ‚ùå Don't use `sys.path.insert()` - project configured with pyproject.toml
- ‚ùå Don't put imports inline or at bottom - **ALWAYS at top**
- ‚ùå Don't initialize provider clients directly (anthropic.Anthropic(), genai.Client())
- ‚ùå Don't save interim steps to `outputs/` (only finals)
- ‚ùå Don't log summary stats without actual data - include decisions, reasons, extractions
- ‚ùå Don't create separate cache files when data fits in StepLogger outputs
- ‚ùå Don't forget to initialize StepLogger and call `.finalize()`
- ‚ùå Don't use generic step names ("Step 1", "LLM Call") - be descriptive
- ‚ùå Don't hard-code API keys - use env vars
- ‚ùå Don't forget timestamps in filenames
- ‚ùå Don't put task-specific code in utils prematurely
- ‚ùå Don't forget to call `load_dotenv()` before importing utils
- ‚ùå Don't use TypedDict, dataclasses, or plain dicts - ALWAYS use Pydantic BaseModel
- ‚ùå Don't forget `--fresh` flag when using resumable mode
- ‚ùå Don't forget to call `mark_item_complete()` after processing each item in resumable scripts
- ‚ùå Don't forget to save intermediate state in `logger.output()` for resumability
